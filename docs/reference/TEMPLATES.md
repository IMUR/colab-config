---
title: "Templates Reference"
description: "Chezmoi template examples and patterns for Co-lab cluster configuration"
version: "1.0"
date: "2025-09-27"
category: "reference"
tags: ["templates", "chezmoi", "examples", "patterns"]
applies_to: ["all_nodes"]
related:
  - "VARIABLES.md"
  - "COMMANDS.md"
  - "../architecture/HYBRID-STRATEGY.md"
---

# Templates Reference

## Chezmoi Template System Overview

The Co-lab cluster uses Chezmoi's template system to provide node-specific configurations while maintaining consistency across the cluster. Templates use Go's `text/template` syntax with additional Chezmoi-specific functions.

## Core Template Files

### .chezmoi.toml.tmpl - Node Detection and Configuration

**Purpose:** Provides node-specific data variables for all other templates.

```toml
[data]
    # Node identification based on hostname
    {{ if eq .chezmoi.hostname "cooperator" -}}
    node_role = "gateway"
    has_gpu = false
    has_nfs_server = true
    has_dns_server = true
    is_gateway = true
    external_access = true
    node_ip = "192.168.254.10"

    {{- else if eq .chezmoi.hostname "projector" -}}
    node_role = "compute"
    has_gpu = true
    gpu_count = 4
    gpu_architecture = "multi_gpu"
    has_archon_service = true
    has_ollama_service = true
    node_ip = "192.168.254.20"

    {{- else if eq .chezmoi.hostname "director" -}}
    node_role = "ml_platform"
    has_gpu = true
    gpu_count = 1
    gpu_architecture = "single_gpu"
    has_ollama_service = true
    node_ip = "192.168.254.30"

    {{- else -}}
    node_role = "unknown"
    has_gpu = false
    {{- end }}

    # Universal cluster settings
    cluster_member = true
    cluster_subnet = "192.168.254.0/24"
    cluster_domain = "ism.la"
    dns_server = "192.168.254.10"
    nfs_server = "192.168.254.10"
    gateway_ip = "192.168.254.10"

    # GPU and CUDA settings (for GPU nodes)
    {{- if or (eq .chezmoi.hostname "projector") (eq .chezmoi.hostname "director") }}
    has_cuda_runtime = true
    has_cuda_toolkit = true
    cuda_version = "12.6"
    cuda_home = "/usr/local/cuda"
    supports_vllm = true
    supports_llama_cpp = true
    supports_pytorch_cuda = true
    has_nvidia_container = true
    docker_gpu_support = true
    {{- else }}
    has_cuda_runtime = false
    has_cuda_toolkit = false
    cuda_version = ""
    cuda_home = ""
    supports_vllm = false
    supports_llama_cpp = false
    supports_pytorch_cuda = false
    has_nvidia_container = false
    docker_gpu_support = false
    {{- end }}

    # Development tools
    has_modern_shell = true
    uses_starship = true
    has_nvm = true
    uses_uv = true
    has_docker = true

    # Tool detection (runtime)
    HAS_EZA = {{ output "command" "-v" "eza" | trim | ne "" }}
    HAS_BAT = {{ output "command" "-v" "bat" | trim | ne "" }}
    HAS_FZF = {{ output "command" "-v" "fzf" | trim | ne "" }}
    HAS_ZOXIDE = {{ output "command" "-v" "zoxide" | trim | ne "" }}
    HAS_ATUIN = {{ output "command" "-v" "atuin" | trim | ne "" }}

    # Python environment
    uses_uv = true
    python_gpu_package_index = "https://download.pytorch.org/whl/cu126"

    # Service management
    has_service_management = true
    systemctl_aliases = true
```

### dot_profile.tmpl - Environment Configuration

**Purpose:** Sets up environment variables and PATH based on node capabilities.

```bash
#!/bin/bash
# .profile - Universal environment configuration
# Generated by chezmoi template for {{ .chezmoi.hostname }}

# Node identification
export CLUSTER_NODE="{{ .chezmoi.hostname }}"
export NODE_ROLE="{{ .node_role }}"
export NODE_IP="{{ .node_ip }}"

# Cluster configuration
export CLUSTER_SUBNET="{{ .cluster_subnet }}"
export CLUSTER_DOMAIN="{{ .cluster_domain }}"
export DNS_SERVER="{{ .dns_server }}"
export NFS_SERVER="{{ .nfs_server }}"

# PATH management function
add_to_path() {
    case ":$PATH:" in
        *":$1:"*) ;;
        *) PATH="$1:$PATH" ;;
    esac
}

# Standard path additions
add_to_path "$HOME/.local/bin"
add_to_path "/usr/local/bin"

{{- if .has_cuda_toolkit }}
# CUDA Development Toolkit
if [ -d "{{ .cuda_home }}/bin" ]; then
    add_to_path "{{ .cuda_home }}/bin"
    export CUDA_HOME="{{ .cuda_home }}"
    export CUDA_VERSION="{{ .cuda_version }}"
fi

# CUDA Libraries
if [ -d "{{ .cuda_home }}/lib64" ]; then
    export LD_LIBRARY_PATH="{{ .cuda_home }}/lib64${LD_LIBRARY_PATH:+":$LD_LIBRARY_PATH"}"
fi

# CUDA Include Path (for compilation)
if [ -d "{{ .cuda_home }}/include" ]; then
    export CUDA_INCLUDE_PATH="{{ .cuda_home }}/include"
    export CPATH="{{ .cuda_home }}/include${CPATH:+":$CPATH"}"
fi
{{- end }}

{{- if .has_nvidia_container }}
# NVIDIA Container Runtime Environment
export NVIDIA_VISIBLE_DEVICES="${NVIDIA_VISIBLE_DEVICES:-all}"
export NVIDIA_DRIVER_CAPABILITIES="${NVIDIA_DRIVER_CAPABILITIES:-compute,utility,graphics}"
{{- end }}

{{- if .has_docker }}
# Docker environment
export DOCKER_BUILDKIT=1
export COMPOSE_DOCKER_CLI_BUILD=1

{{- if .has_nvidia_container }}
# Docker GPU runtime
export DOCKER_DEFAULT_RUNTIME=nvidia
{{- end }}
{{- end }}

{{- if .uses_uv }}
# UV Python Package Management with CUDA
export UV_EXTRA_INDEX_URL="{{ .python_gpu_package_index }}"
export UV_CUDA_VERSION="{{ .cuda_version }}"
{{- end }}

# Node-specific GPU optimizations
{{- if eq .gpu_architecture "multi_gpu" }}
# Multi-GPU optimizations (projector)
export CUDA_DEVICE_ORDER="PCI_BUS_ID"
export NCCL_DEBUG=INFO
export OMP_NUM_THREADS=8
{{- else if eq .gpu_architecture "single_gpu" }}
# Single GPU optimizations (director)
export CUDA_DEVICE_ORDER="FASTEST_FIRST"
export OMP_NUM_THREADS=4
{{- end }}

# Tool detection flags
{{- range $tool, $available := dict "EZA" .HAS_EZA "BAT" .HAS_BAT "FZF" .HAS_FZF "ZOXIDE" .HAS_ZOXIDE "ATUIN" .HAS_ATUIN }}
export HAS_{{ $tool }}={{ if $available }}true{{ else }}false{{ end }}
{{- end }}

# NVM configuration
export NVM_DIR="$HOME/.nvm"

# Load .bashrc if running bash and not already loaded
if [ -n "$BASH_VERSION" ] && [ -f "$HOME/.bashrc" ]; then
    . "$HOME/.bashrc"
fi
```

### dot_bashrc.tmpl - Bash Shell Configuration

**Purpose:** Configures bash with modern tools and node-specific aliases.

```bash
#!/bin/bash
# .bashrc - Bash configuration
# Generated by chezmoi template for {{ .chezmoi.hostname }} ({{ .node_role }})

# If not running interactively, don't do anything
case $- in
    *i*) ;;
      *) return;;
esac

# History configuration
HISTCONTROL=ignoreboth
HISTSIZE=10000
HISTFILESIZE=20000
shopt -s histappend

# Check window size after each command
shopt -s checkwinsize

# Make less more friendly for non-text input files
[ -x /usr/bin/lesspipe ] && eval "$(SHELL=/bin/sh lesspipe)"

# Set up colors
if [ -x /usr/bin/dircolors ]; then
    test -r ~/.dircolors && eval "$(dircolors -b ~/.dircolors)" || eval "$(dircolors -b)"
fi

# Load shared template includes
{{- template ".chezmoitemplate.nvm-loader.sh" . }}

# Modern tool integrations
{{- if .HAS_STARSHIP }}
# Starship prompt
if command -v starship >/dev/null 2>&1; then
    eval "$(starship init bash)"
else
{{- end }}
    # Fallback colorized prompt
    PS1='\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ '
{{- if .HAS_STARSHIP }}
fi
{{- end }}

{{- if .HAS_FZF }}
# FZF integration
if command -v fzf >/dev/null 2>&1; then
    source /usr/share/fzf/key-bindings.bash 2>/dev/null || true
    source /usr/share/fzf/completion.bash 2>/dev/null || true
    export FZF_DEFAULT_OPTS='--height 40% --layout=reverse --border'
fi
{{- end }}

{{- if .HAS_ZOXIDE }}
# Zoxide (smart cd)
if command -v zoxide >/dev/null 2>&1; then
    eval "$(zoxide init bash)"
fi
{{- end }}

{{- if .HAS_ATUIN }}
# Atuin (shell history)
if command -v atuin >/dev/null 2>&1; then
    eval "$(atuin init bash)"
fi
{{- end }}

# Aliases and functions
{{- if .HAS_EZA }}
# Modern ls replacement
alias ls='eza'
alias ll='eza -la'
alias la='eza -la'
alias lt='eza --tree'
alias lg='eza -la --git'
{{- else }}
# Traditional ls with colors
alias ls='ls --color=auto'
alias ll='ls -alF'
alias la='ls -A'
alias l='ls -CF'
{{- end }}

{{- if .HAS_BAT }}
# Syntax-highlighted cat
alias cat='bat'
alias less='bat'
{{- end }}

# Common aliases
alias grep='grep --color=auto'
alias fgrep='fgrep --color=auto'
alias egrep='egrep --color=auto'

{{- if .has_gpu }}
# GPU monitoring and management
alias gpus='nvidia-smi'
alias gpu-watch='watch -n 1 nvidia-smi'
alias gpu-mem='nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader'
alias gpu-util='nvidia-smi --query-gpu=utilization.gpu,utilization.memory --format=csv,noheader'
alias gpu-temp='nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader'
alias gpu-power='nvidia-smi --query-gpu=power.draw,power.limit --format=csv,noheader'

{{- if .has_cuda_toolkit }}
# CUDA development
alias cuda-version='nvcc --version'
alias cuda-devices='nvidia-smi -L'
alias cuda-compile='nvcc -O3 -arch=sm_61'
{{- end }}
{{- end }}

{{- if .has_docker }}
# Docker aliases
alias dps='docker ps --format "table {{`{{.Names}}`}}\t{{`{{.Status}}`}}\t{{`{{.Ports}}`}}"'
alias dimg='docker images --format "table {{`{{.Repository}}`}}\t{{`{{.Tag}}`}}\t{{`{{.Size}}`}}"'
alias dlog='docker logs -f'
alias dexec='docker exec -it'

{{- if .has_nvidia_container }}
# Docker GPU integration
alias docker-gpu='docker run --gpus all'
alias docker-gpu-it='docker run -it --rm --gpus all'
alias docker-gpu-test='docker run --rm --gpus all nvidia/cuda:{{ .cuda_version }}-base-ubuntu20.04 nvidia-smi'
{{- end }}
{{- end }}

{{- if .cluster_member }}
# Cluster management
alias cluster-ping='ansible all -m ping'
alias cluster-status='for node in crtr prtr drtr; do echo "=== $node ==="; ssh $node "uptime; free -h"; done'
alias cluster-update='chezmoi update'
{{- end }}

{{- if .has_service_management }}
# Service management aliases
alias svc-status='systemctl status'
alias svc-start='sudo systemctl start'
alias svc-stop='sudo systemctl stop'
alias svc-restart='sudo systemctl restart'
alias svc-enable='sudo systemctl enable'
alias svc-disable='sudo systemctl disable'
alias svc-logs='sudo journalctl -u'
alias svc-follow='sudo journalctl -u -f'
{{- end }}

{{- if .has_archon_service }}
# Archon service management
alias archon-status='systemctl status archon.service'
alias archon-logs='sudo journalctl -u archon.service -f'
alias archon-restart='sudo systemctl restart archon.service'
alias archon-containers='docker ps --filter "name=archon" --format "table {{`{{.Names}}`}}\t{{`{{.Status}}`}}"'
{{- end }}

{{- if .has_ollama_service }}
# Ollama service management
alias ollama-status='systemctl status ollama.service'
alias ollama-logs='sudo journalctl -u ollama.service -f'
alias ollama-restart='sudo systemctl restart ollama.service'
{{- end }}

{{- if .is_gateway }}
# Gateway-specific aliases
alias check-dns='dig @localhost example.com'
alias pihole-status='systemctl status pihole-FTL'
alias nfs-status='showmount -e'
alias caddy-reload='sudo systemctl reload caddy'
{{- end }}

{{- if eq .gpu_architecture "multi_gpu" }}
# Multi-GPU specific aliases (projector)
alias gpu-topology='nvidia-smi topo -m'
alias gpu-reset-all='sudo nvidia-smi -r'
alias distributed-test='python -m torch.distributed.launch --nproc_per_node={{ .gpu_count }}'
{{- end }}

# Node-specific welcome message
echo "Welcome to {{ .chezmoi.hostname }} ({{ .node_role }})"
{{- if .has_gpu }}
echo "GPU: {{ .gpu_count }} device(s) available"
{{- end }}
{{- if .is_gateway }}
echo "Gateway services: DNS, NFS, Proxy"
{{- end }}
```

### dot_zshrc.tmpl - Zsh Shell Configuration

**Purpose:** Configures zsh with modern tools and node-specific aliases (similar to bash but zsh-specific).

```bash
#!/bin/zsh
# .zshrc - Zsh configuration
# Generated by chezmoi template for {{ .chezmoi.hostname }} ({{ .node_role }})

# History configuration
HISTSIZE=10000
SAVEHIST=20000
HISTFILE=~/.zsh_history
setopt HIST_IGNORE_DUPS
setopt HIST_IGNORE_SPACE
setopt HIST_FIND_NO_DUPS
setopt SHARE_HISTORY
setopt APPEND_HISTORY

# Completion system
autoload -Uz compinit
compinit

# Load shared template includes
{{- template ".chezmoitemplate.nvm-loader.sh" . }}

# Modern tool integrations
{{- if .HAS_STARSHIP }}
# Starship prompt
if command -v starship >/dev/null 2>&1; then
    eval "$(starship init zsh)"
fi
{{- end }}

{{- if .HAS_FZF }}
# FZF integration
if command -v fzf >/dev/null 2>&1; then
    source /usr/share/fzf/key-bindings.zsh 2>/dev/null || true
    source /usr/share/fzf/completion.zsh 2>/dev/null || true
    export FZF_DEFAULT_OPTS='--height 40% --layout=reverse --border'
fi
{{- end }}

{{- if .HAS_ZOXIDE }}
# Zoxide (smart cd)
if command -v zoxide >/dev/null 2>&1; then
    eval "$(zoxide init zsh)"
fi
{{- end }}

{{- if .HAS_ATUIN }}
# Atuin (shell history)
if command -v atuin >/dev/null 2>&1; then
    eval "$(atuin init zsh)"
fi
{{- end }}

# Include the same aliases as bash
{{- template ".chezmoitemplate.bash-aliases.sh" . }}

# Zsh-specific configurations
setopt AUTO_CD              # cd by typing directory name
setopt CORRECT              # command correction
setopt COMPLETE_IN_WORD     # complete from both ends of word
setopt GLOB_DOTS            # include dotfiles in glob

# Key bindings
bindkey "^[[1;5C" forward-word      # Ctrl+Right
bindkey "^[[1;5D" backward-word     # Ctrl+Left
bindkey "^[[3~" delete-char         # Delete key

# Node-specific zsh welcome
echo "🚀 {{ .chezmoi.hostname }} ({{ .node_role }}) - Zsh Ready"
{{- if .has_gpu }}
echo "⚡ {{ .gpu_count }} GPU(s) detected"
{{- end }}
```

## Shared Template Includes

### .chezmoitemplate.nvm-loader.sh - NVM Loading

**Purpose:** Shared template for loading NVM across bash and zsh.

```bash
{{/* .chezmoitemplate.nvm-loader.sh - Shared NVM loading logic */}}
# NVM (Node Version Manager) setup
if [ -d "$NVM_DIR" ]; then
    # Load nvm
    [ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"

    # Load nvm bash completion
    [ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"
fi

# Auto-load .nvmrc files
autoload_nvmrc() {
    if [[ -f .nvmrc && -r .nvmrc ]]; then
        nvm use
    elif [[ $(nvm version) != $(nvm version default) ]]; then
        echo "Reverting to nvm default version"
        nvm use default
    fi
}

# Hook for zsh
if [[ -n "$ZSH_VERSION" ]]; then
    autoload -U add-zsh-hook
    add-zsh-hook chpwd autoload_nvmrc
    autoload_nvmrc
fi

# Hook for bash
if [[ -n "$BASH_VERSION" ]]; then
    cd() { builtin cd "$@" && autoload_nvmrc; }
fi
```

### .chezmoitemplate.bash-aliases.sh - Shared Aliases

**Purpose:** Common aliases shared between bash and zsh.

```bash
{{/* .chezmoitemplate.bash-aliases.sh - Shared alias definitions */}}
# File operations
{{- if .HAS_EZA }}
alias ls='eza'
alias ll='eza -la'
alias la='eza -la'
alias lt='eza --tree'
alias lg='eza -la --git'
{{- else }}
alias ls='ls --color=auto'
alias ll='ls -alF'
alias la='ls -A'
alias l='ls -CF'
{{- end }}

{{- if .HAS_BAT }}
alias cat='bat'
alias less='bat'
{{- end }}

# Search and navigation
alias grep='grep --color=auto'
alias fgrep='fgrep --color=auto'
alias egrep='egrep --color=auto'

{{- if .HAS_FZF }}
alias preview='fzf --preview "bat --color=always {}"'
{{- end }}

# Git shortcuts
alias gs='git status'
alias ga='git add'
alias gc='git commit'
alias gp='git push'
alias gl='git log --oneline'

# System information
alias df='df -h'
alias du='du -h'
alias free='free -h'
alias ps='ps aux'

# Network
alias ports='netstat -tuln'
alias listening='ss -tuln'

# Safety aliases
alias rm='rm -i'
alias cp='cp -i'
alias mv='mv -i'

# Convenience
alias ..='cd ..'
alias ...='cd ../..'
alias ....='cd ../../..'
alias ~='cd ~'
alias -- -='cd -'

# Make directories and change to them
mkcd() { mkdir -p "$1" && cd "$1"; }
```

## Advanced Template Patterns

### Conditional Configuration Blocks

**Multi-condition templates:**
```bash
{{- if and .has_gpu .has_docker }}
# GPU + Docker specific configuration
alias docker-gpu-shell='docker run -it --rm --gpus all -v $(pwd):/workspace nvidia/pytorch:latest bash'
{{- end }}

{{- if or (eq .node_role "compute") (eq .node_role "ml_platform") }}
# Compute node specific aliases
alias monitor-gpu='watch -n 1 "nvidia-smi; echo; docker stats --no-stream"'
{{- end }}

{{- if and .cluster_member (not .is_gateway) }}
# Non-gateway cluster members
alias connect-gateway='ssh {{ .gateway_ip }}'
{{- end }}
```

### Template Functions and Pipes

**Using template functions:**
```bash
# Current date and hostname in template
# Generated on {{ now | date "2006-01-02 15:04:05" }} for {{ .chezmoi.hostname }}

{{- range $service := list "docker" "nginx" "mysql" }}
{{- if (output "systemctl" "is-active" $service | trim) | eq "active" }}
alias {{ $service }}-status='systemctl status {{ $service }}'
{{- end }}
{{- end }}

# Tool detection with command output
{{- $tools := dict "eza" "ls" "bat" "cat" "fzf" "find" }}
{{- range $tool, $fallback := $tools }}
{{- if (output "command" "-v" $tool | trim) }}
export HAS_{{ upper $tool }}=true
{{- else }}
export HAS_{{ upper $tool }}=false
{{- end }}
{{- end }}
```

### Configuration File Templates

**Starship configuration (dot_config/starship.toml.tmpl):**
```toml
# Starship configuration for {{ .chezmoi.hostname }}
format = """
$hostname\
$directory\
$git_branch\
$git_status\
{{- if .has_gpu }}
$custom.gpu\
{{- end }}
$character"""

[hostname]
ssh_only = false
format = "[$hostname](bold {{ if eq .node_role "gateway" }}blue{{ else if eq .node_role "compute" }}red{{ else }}green{{ end }}) "

[directory]
truncation_length = 3
format = "[$path]($style)[$read_only]($read_only_style) "

{{- if .has_gpu }}
[custom.gpu]
command = "nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits | head -1"
when = "command -v nvidia-smi"
format = "[🚀$output%]($style) "
style = "yellow"
{{- end }}

[character]
success_symbol = "[➜](bold green)"
error_symbol = "[➜](bold red)"
```

## Template Testing and Debugging

### Template Validation Commands

**Test template rendering:**
```bash
# Test data availability
chezmoi data

# Test specific template expressions
chezmoi execute-template '{{ .node_role }}'
chezmoi execute-template '{{ if .has_gpu }}GPU available{{ end }}'

# Test template file rendering
chezmoi cat ~/.bashrc
chezmoi diff ~/.bashrc

# Test with different hostname
chezmoi execute-template --init --promptString hostname=testnode < .chezmoi.toml.tmpl
```

### Common Template Patterns

**Safe conditional execution:**
```bash
# Check if command exists before using
{{- if (lookPath "docker") }}
alias dps='docker ps'
{{- end }}

# Check if file exists
{{- if (stat "/usr/local/cuda") }}
export CUDA_HOME="/usr/local/cuda"
{{- end }}

# Default values
{{- $gpu_count := .gpu_count | default 0 }}
{{- if gt $gpu_count 0 }}
echo "GPU count: {{ $gpu_count }}"
{{- end }}
```

**Loop constructs:**
```bash
# Loop over tools
{{- range $tool := list "git" "vim" "tmux" "htop" }}
{{- if (lookPath $tool) }}
# {{ $tool }} is available
{{- end }}
{{- end }}

# Loop over nodes
{{- range $node := list "cooperator" "projector" "director" }}
alias ssh-{{ $node }}='ssh {{ $node }}.ism.la'
{{- end }}
```

**String manipulation:**
```bash
# String operations
{{- $hostname := .chezmoi.hostname }}
{{- $shortname := $hostname | trunc 4 }}
{{- $uppercase := $hostname | upper }}

# Path manipulation
{{- $cudabin := .cuda_home | printf "%s/bin" }}
{{- $cudapath := list .cuda_home "bin" | join "/" }}
```

This template reference provides comprehensive examples and patterns for creating flexible, node-aware configurations in the Co-lab cluster environment.